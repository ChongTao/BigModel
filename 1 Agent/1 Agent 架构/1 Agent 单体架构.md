# ReAct 

[ReACT](https://arxiv.org/pdf/2210.03629) 提出让大语言模型（LLM）**交错地生成“推理（reasoning）轨迹”和“动作（actions）”**，使模型在做决策/问答时既能“思考又能动手去查询/操作”，从而提高准确性、减少幻觉（hallucination），并产生更可解释的人类可读解题轨迹。

ReAct 允许大模型与外部工具交互来获取额外信息，从而给出更可靠和实际的回应。

CoT提示显示了大模型执行推理过程和生成的原因 [(Wei 等人，[2022)](https://arxiv.org/abs/2201.11903)。但它因缺乏和外部世界的接触或无法更新自己的知识，而导致幻觉和错误等问题。

ReAct是将Reason+Act结合，下图展示有无ReAct对推理结果的对比。
