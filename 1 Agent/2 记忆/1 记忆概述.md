# 一 为什么需要 “记忆”

大模型本身是一个**条件概率模型**，它不具备持续状态，每次推理都是“**从零开始**”。如果没有记忆将导致：

- 上下文丢失，对话断裂。
- 无法个性化，用户体验差。
- 无法持续对话或多轮交互，需要每次都重复说明。

记忆就是让大模型能够跨越单次输入上下文，持续记住和利用信息，而不仅仅依赖于有限的上下文窗口。记忆是智能体实现长期规划、持续学习和个性化交互的基石。

> CoT/ReAct 解决了 Agent 的“行动闭环”，而 Memory 解决了 Agent 的“经验积累”。

# 二 记忆的常见分类

## 2.1 短期记忆（Short-term Memory）

- **定义**：类似人类的工作记忆，依赖于模型的上下文窗口（context window）。
- **特点**：随着对话长度增加，旧信息会被截断遗忘。
- **技术手段**：长上下文扩展（RoPE 插值、ALiBi、Attention Sink、滑动窗口注意力等）。
- **缺点**：成本高，不适合保存长期知识。

![](https://www.dailydoseofds.com/content/images/2025/04/image-17.png)

## 2.2 长期记忆（Long-term Memory）

- **定义**：跨会话、跨任务的记忆，能永久保存。
- **存储方式**：通常是**外部存储**（数据库、向量库、KV 存储）。
- **检索机制**：在新请求时，将历史内容做 embedding 向量化，按相似度检索，再拼接进 prompt。
- **常用技术**：
  - RAG（Retrieval-Augmented Generation）
  - 向量数据库（FAISS, Milvus, Weaviate, Pinecone 等）
  - 基于关键词 / 主题的索引
- 多模态记忆对齐

![](https://www.dailydoseofds.com/content/images/2025/04/image-19.png)

## 2.3 工作记忆（Working Memory）

- **定义**：介于短期与长期之间，类似一个临时缓存。
- **用途**：在多轮任务执行中保存中间步骤结果，供后续调用。
- **实现**：
  - 会话缓存（conversation buffer）
  - 知识图谱临时节点
  - scratchpad（推理时的草稿区，比如 Chain-of-Thought 的显式记录）

## 2.4 情节记忆（Episodic Memory）

- **定义**：存储用户与模型的“交互事件”，按会话 / 时间组织。
- **作用**：模型能回忆“上次我们聊到哪里”。
- **实现**：事件时间戳 + 向量表示 + 语义检索。

## 2.5 语义记忆（Semantic Memory）

- **定义**：将信息抽象总结为“知识点”。
- **例子**：用户常说“我喜欢用 Go 写后端” → 存成一个偏好知识条目。
- **实现**：定期对对话日志做总结 → 存到知识库。
- **优点**：节省空间，便于泛化。

## 2.6 程序化 / 工具化记忆（Tool-based Memory）

- 模型并不直接“记住”，而是通过调用外部工具访问：
  - 个人笔记系统（Notion, Obsidian）
  - 日历 / 待办事项 API
  - CRM 系统（存储用户信息）



# 三 记忆框架

下面是专门研究Memory的框架：

- [Memobase](./3 Memobase.md)
- [Mem0](./4 Mem0.md)
- [MemGPT](./5 MemGPT.md)
