# 一 NLP简介

**自然语言处理（Natural Language Processing, NLP）**是计算机科学、人工智能和语言学的交叉领域，致力于让计算机能够理解、处理和生成人类的自然语言（参考https://www.runoob.com/nlp/nlp-tutorial.html）。

**核心目标：**

- **理解**：让计算机能够理解人类语言的含义
- **处理**：对文本和语音进行分析、转换和操作
- **生成**：让计算机能够产生自然、流畅的人类语言

## 1.1 核心任务

- 基础任务
  - **分词**：将文本分解为有意义的单元。
  - **词性标注**：标识每个词的语法类别。
  - **句法分析（Parsing）**：分析句子的语法结构。
  - **命名实体识别（NER）**：识别人名、地名、机构名等。
- 理解任务
  - **语义角色标注**：识别句子中的语义关系
  - **共指消解**：确定文本中指向同一实体的不同表达
  - **关系抽取**：识别实体间的语义关系
  - **事件抽取**：从文本中抽取事件信息

- 应用任务

  - **文本分类**：将文本归类到预定义类别

  - **情感分析**：判断文本的情感倾向

  - **机器翻译**：将一种语言翻译成另一种语言

  - **文本摘要**：生成文本的简洁总结

  - **问答系统**：根据问题检索或生成答案

# 2 文本预处理

文本预处理是自然语言处理（NLP）中的基础且关键步骤，它将原始的非结构化文本数据转化为适合机器学习模型处理的格式。

本文将系统介绍文本预处理的三大核心环节：文本清洗、分词和词性标注。

## 2.1 文本清洗

文本清洗是预处理的第一步，目的是去除文本中的噪声数据，提高后续处理的准确性。

- 编码格式处理：中文、英文归一化UTF-8
- 特殊字符处理：HTML、表情符号、控制字符和特殊标点
- 噪声数据去除：广告、版权、拼写错误

## 2.2 分词

分词是将连续文本分割成有意义的语言单元（token）的过程，不同语言需要不同的分词方法。

- 英文分词方法：主要按照空格和标点分割。
- 中文分词技术：基于词典的分词、基于统计的分词、基于深度学习的分词
- 字词分词：BPE、NLTK、JieBa

## 2.3 词性标注：理解词语的语法角色

词性标注（Part-of-Speech Tagging）是为分词结果中的每个词语标注其词性类别的过程。

- 词性标注：理解句子结构、消除词义歧义、支持更高级NLP任务。
- 常见词性体系：NN、VB、RB等
- 自动词性标注方法：基于规则的方法、基于统计的方法、基于深度学习的方法。
- 词性标注的评估指标：准确率、未知词准确率

# 3 文本表示方法

文本表示是自然语言处理（NLP）中的基础任务，它将非结构化的文本数据转化为计算机可以处理的数值形式。

## 3.1 传统文本表示

- 词袋模型：最简单的文本表示方法之一，它将文本视为一个无序的词汇集合。
- TF-IDF：TF-IDF（Term Frequency-Inverse Document Frequency）是对词袋模型的改进，考虑了词语在整个语料库中的重要性。
  - TF（词频）：`词在文档中出现的次数 / 文档总词数`
  - IDF（逆文档频率）：`log(文档总数 / 包含该词的文档数)`
  - TF-IDF = TF × IDF
- N-gram模型：词语的顺序信息，通过连续n个词的组合来表示文本。
  - Unigram (1-gram)：单个词
  - Bigram (2-gram)：两个连续词的组合
  - Trigram (3-gram)：三个连续词的组合

- 词向量表示：
  - word2Vec：一种基于神经网络的词向量表示方法。
    - **CBOW（Continuous Bag of Words）**：通过上下文预测当前词。
    - **Skip-gram**：通过当前词预测上下文。
  - GloVe词向量：结合了全局统计信息和局部上下文窗口的优点。
- FastText：Facebook 开发的词向量模型，特点是考虑子词（subword）信息。

## 3.2 上下文感知

- ELMo模型：是最早的上下文相关词表示方法之一。
  - 基于双向LSTM语言模型
  - 词语的表示取决于整个输入句子
  - 生成多层表示（可以组合不同层次的语义）
- BERT：预训练语言模型。
  - Transformer 架构
  - 掩码语言模型（MLM）训练目标
  - 下一句预测（NSP）任务
- 文档级表示
  - Doc2Vec：是 Word2Vec 的扩展，可以直接学习文档的向量表示。
  - 句向量与文档向量
    1. **平均法**：对词向量取平均
    2. **SIF**：平滑逆频率加权平均
    3. **BERT句向量**：使用[CLS]标记或平均所有词向量
  - 主题模型：潜在狄利克雷分配（LDA）是一种无监督的主题建模方法。

# 4 文本分类

文本分类(Text Classification)是自然语言处理(NLP)中最基础也是最重要的任务之一。它的目标是将给定的文本文档自动归类到一个或多个预定义的类别中。

## 4.1 应用场景

文本分类在现代社会中有着广泛的应用：

1. **情感分析**：判断评论是正面还是负面
2. **垃圾邮件过滤**：区分正常邮件和垃圾邮件
3. **新闻分类**：将新闻归类到体育、财经、科技等板块
4. **意图识别**：理解用户查询的真实意图
5. **医疗诊断**：根据症状描述分类疾病类型

## 4.2 基本流程

文本预处理->特征提取->分类模型选择。

# 5 情感分析

情感分析(Sentiment Analysis)是自然语言处理(NLP)领域中最经典且应用最广泛的任务之一。它通过计算技术自动识别、提取和分析文本中的主观信息，判断作者对特定主题、产品或服务的态度是正面、负面还是中性。

## 5.1 基于词典的情感分析方法

基于词典的方法是最传统的情感分析技术，主要依赖预构建的情感词典。

- 情感词典：包含带有情感极性和强度的词语集合。
- **强度调节器**：处理程度副词和否定词的影响

## 5.2 基于机器学习的情感分析方法

机器学习方法通过从标注数据中学习模式来进行情感分析。

1. **词袋模型(BOW)**：文本表示为词语出现频率的向量
2. **TF-IDF**：考虑词语在文档中的重要性
3. **N-gram特征**：捕获局部词语序列模式
4. **情感词典特征**：结合词典方法的优势

## 5.3 细粒度情感分析

细粒度情感分析是更高级的情感分析任务，旨在识别文本中提到的特定方面及其对应的情感。

1. **方面提取**：识别文本中讨论的实体或属性
2. **情感分类**：对每个识别出的方面进行情感判断

# 6 命令实体识别（NER）

命名实体识别是自然语言处理（NLP）中的一项基础任务，它的目标是识别文本中具有特定意义的实体，并将其分类到预定义的类别中。

- **命名实体**：文本中表示特定对象的专有名词
- **实体类别**：常见类型包括人名、地名、组织机构名、时间、日期、货币等

## 6.1 NER的应用场景

1. **信息提取**：从新闻中提取关键人物和事件
2. **搜索引擎优化**：增强搜索结果的语义理解
3. **客户支持**：自动识别用户查询中的关键实体
4. **医疗领域**：识别病历中的药物名称和疾病术语

## 6.2 NER的技术实现

### 基本方法分类

| 方法类型 | 描述                 | 优缺点               |
| :------- | :------------------- | :------------------- |
| 规则匹配 | 基于预定义规则和词典 | 高精度但覆盖率低     |
| 统计学习 | 使用传统机器学习模型 | 需要特征工程         |
| 深度学习 | 基于神经网络模型     | 高性能但需要大量数据 |

## 6.3 NER的评估指标

1. **精确率（Precision）**：识别正确的实体占所有识别实体的比例
2. **召回率（Recall）**：识别正确的实体占所有实际实体的比例
3. **F1分数**：精确率和召回率的调和平均数

# 7 关系抽取

关系抽取是自然语言处理(NLP)中的一个重要任务，旨在从非结构化文本中识别实体之间的语义关系。简单来说，就是从句子中找出"谁"和"谁"之间有什么"关系"。关系抽取的核心要素：

- **实体识别**：首先需要识别文本中的命名实体
- **关系分类**：然后判断这些实体之间存在什么类型的关系
- **关系表示**：最后以结构化形式表示这些关系

## 7.1 关系抽取方法

- 基于规则的方法
- 监督学习方法：支持向量机、条件随机场、深度学习模型
- 半监督/远程监督方法
- 基于预训练语言模型的方法：BERT、GPT

## 7.2 关系抽取的关键技术

- 实体识别：NER、实体链接
- 关系分类：二元关系、n元关系、关系层次结构

## 7.3 关系抽取问题

- 语言多样性
- 实体歧义
- 长距离依赖
- 数据稀疏
- 领域适应

# 8 文本相似度计算

文本相似度计算是自然语言处理(NLP)中的一项基础任务，旨在量化两个文本片段之间的相似程度。这项技术在信息检索、问答系统、抄袭检测、推荐系统等多个领域都有广泛应用。

- **语义相似度**：衡量文本在含义上的接近程度
- **字面相似度**：衡量文本在表面词汇上的重叠程度
- **向量空间模型**：将文本表示为高维空间中的向量
- **距离度量**：计算向量之间的距离或相似度

## 8.1 文本相似度计算方法

- 基于词频的方法：BOW、TF-IDF
- 基于词向量的方法：Word2Vec、句子向量
- 基于预训练模型方法：BERT

## 8.2 相似度度量指标

- 余弦相似度、欧式举例、曼哈顿举例

# 9 RNN

循环神经网络（Recurrent Neural Network，RNN） 是一种专门处理序列数据（如文本、语音、时间序列）的神经网络。

与传统的前馈神经网络不同，RNN 具有"记忆"能力，能够保存之前步骤的信息。

循环神经网络能够利用前一步的隐藏状态（Hidden State）来影响当前步骤的输出，从而捕捉序列中的时序依赖关系。

![](https://www.runoob.com/wp-content/uploads/2025/06/1rnn-scaled.png)

## 9.1 核心思想

RNN 的核心在于**循环连接**（Recurrent Connection），即网络的输出不仅取决于当前输入，还取决于之前所有时间步的输入。这种结构使 RNN 能够处理任意长度的序列数据。

**传统神经网络**：输入和输出是独立的（例如图像分类，单张图片之间无关联）。

**RNN**：通过**循环连接**（Recurrent Connection）将上一步的隐藏状态传递到下一步，形成"记忆"。

- 每一步的输入 = 当前数据 + 上一步的隐藏状态。
- 输出不仅依赖当前输入，还依赖之前所有步骤的上下文

RNN 在每个时间步 t 执行以下计算：

1. 接收当前输入 xₜ 和前一时刻的隐藏状态 hₜ₋₁
2. 计算新的隐藏状态 hₜ = f(Wₕₕ·hₜ₋₁ + Wₓₕ·xₜ + b)
3. 产生输出 yₜ = g(Wₕᵧ·hₜ + c)

其中 f 和 g 通常是激活函数（如 tanh 或 softmax）。

## 9.2 长短期记忆网络

LSTM（Long Short-Term Memory）是 RNN 的一种改进架构，专门设计来解决标准 RNN 的长期依赖问题。

LSTM 引入了三个门控机制和一个记忆单元：

- 输入门：控制新信息的流入
- 遗忘门：决定丢弃哪些旧信息
- 输出门： 控制输出的信息量
- 记忆单元：保存长期状态

## 9.3 门控循环单元

GRU（Gated Recurrent Unit）是 LSTM 的简化版本，在保持相似性能的同时减少了参数数量。

GRU 合并了 LSTM 的某些组件：

- 更新门：决定保留多少旧信息
- 重置门：决定如何组合新旧信息
- 候选激活：基于重置门计算的新状态

## 9.4 双向RNN

双向 RNN 通过同时考虑过去和未来的上下文信息，增强了序列建模能力。

Bi-RNN 包含两个独立的 RNN 层：

1. 前向层：按时间顺序处理序列
2. 反向层：按时间逆序处理序列

# 10 注意力机制

注意力机制是深度学习中的一种重要技术，它模仿了人类视觉和认知过程中的注意力分配方式。就像你在阅读时会不自觉地将注意力集中在关键词上一样，注意力机制让神经网络能够动态地关注输入数据中最相关的部分。

注意力机制的核心思想是：**根据输入的不同部分对当前任务的重要性，动态分配不同的权重**。这种权重分配不是固定的，而是根据上下文动态计算的。

注意力机制通常可以表示为：

```latex
Attention(Q, K, V) = softmax(QK^T/√d_k)V
```

- Q (Query)：当前需要计算输出的查询项
- K (Key)：用于与查询项匹配的键
- V (Value)：与键对应的实际值
- d_k：键的维度，用于缩放点积结果

注意力机制目的：

1. **解决长距离依赖问题**：传统RNN难以捕捉远距离词语间的关系
2. **并行计算能力**：相比RNN的顺序处理，注意力可以并行计算
3. **可解释性**：注意力权重可以直观展示模型关注的重点

## 10.1 自注意力机制

自注意力是注意力机制的一种特殊形式，它允许输入序列中的每个元素都与序列中的所有其他元素建立联系。

工作原理

1. 对输入序列中的每个元素，计算其与所有元素的相似度得分
2. 使用softmax函数将这些得分转换为权重(0-1之间)
3. 用这些权重对对应的值进行加权求和，得到输出

## 10.2 多头注意力

多头注意力是自注意力的扩展，它将注意力机制并行执行多次，然后将结果拼接起来。

结构组成：

1. **多个注意力头**：通常使用8个或更多并行的注意力头
2. **线性变换层**：每个头有自己的Q、K、V变换矩阵
3. **拼接和输出**：将各头的输出拼接后通过线性层

# 11 Transformer架构

Transformer 架构是一种基于自注意力机制（Self-Attention）的深度学习模型，由 Google 团队在 2017 年的论文[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762)中首次提出。

Transformer 彻底改变了自然语言处理（NLP）领域，并成为现代大语言模型（如GPT、BERT等）的核心基础。

Transformer 与循环神经网络（RNN）类似，旨在处理自然语言等顺序输入数据，适用于机器翻译、文本摘要等任务。然而，与 RNN 不同，Transformer 无需逐步处理序列，而是可以一次性并行处理整个输入。

## 11.1 Transformer结构图

![](https://www.runoob.com/wp-content/uploads/2025/06/Transformer_full_architecture.png)

1. 输入处理：将输入的单词转换成数字向量
2. 编码器
   - **Multi-Headed Self-Attention**：让模型同时关注输入中的所有单词，并计算它们之间的关系。
   - **Norm**：稳定训练过程，防止数值过大或过小
   - **Feed-Forward Network**：对每个单词的表示进行进一步加工
3.  解码器
   - **Masked Multi-Headed Self-Attention**：训练时防止模型"作弊"（只能看到当前和之前的单词，不能看未来的）
   - **Multi-Headed Cross-Attention**：让解码器询问编码器："关于输入，我应该重点关注什么？"
   - **Norm 和 Feed-Forward Network**：与编码器类似，对解码器的表示进行归一化和深度处理。
4. 输出
   - **Linear**：将解码器的输出映射到词表。

## 11.2 Translate核心思想

完全依赖注意力机制（无需循环或卷积结构）来捕捉输入序列中的全局依赖关系，从而实现高效的并行计算和更强的长距离依赖建模。

# 12 序列到序列模型

序列到序列模型是自然语言处理(NLP)中的一种重要架构，专门用于将一个序列转换为另一个序列的任务。这种模型的核心思想是接受一个长度可变的输入序列，生成一个长度可变的输出序列。

Seq2Seq模型属于**编码器-解码器(Encoder-Decoder)**架构：

- **编码器**：将输入序列编码为一个固定长度的上下文向量(context vector)
- **解码器**：根据上下文向量逐步生成输出序列