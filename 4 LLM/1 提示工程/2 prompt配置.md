# 模型配置

**Temperature**：是大语言模型生成控制中最核心的超参数之一。 它直接影响模型输出的「创造性」、「确定性」和「多样性」。其值越小，模型返回的结果越确定，否则结果更加随机。在不同任务中配置不同，例如代码生成中建议配置0.0~0.2，而创作类讲义配置1.0。

**top_p**(nucleus sampling)：和 `temperature` 一样都控制 **输出的随机性与多样性**，如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。

**max_tokens**：模型一次生成的最大 token 数。

**presence_penalty**：惩罚已出现过的 token，鼓励话题多样化

**frequency_penalty**：惩罚重复出现的 token，减少重复句。

**stop**：指定模型遇到哪些 token/字符串时停止输出