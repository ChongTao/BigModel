# 一 上下文压缩

**上下文压缩（Context Compression）** 是指在上下文长度有限或对话持续变长时，通过技术手段对历史内容进行“摘要化、结构化或选择性保留”，以减少 token 使用、缓解注意力稀释、避免 Context Rot，同时保持任务所需的语义信息不丢失。

## 1.1 为什么需要上下文压缩

模型 context window 有限，大量历史会导致：

- 计算成本高
- 注意力稀释（导致模型忽略重要内容）
- Context Rot（上下文腐败）
- 模型开始混乱、偏离任务
- 不必要的token浪费

上下文压缩的目标是：

- 保留关键语义
- 丢弃噪音
- 降低 token
- 维持对话质量



# 2 上下文压缩的分类

## 2.1 抽象压缩（Abstractive Compression）

类似“模型写一个总结”。特点是“重写、概括、语义保留”。如：

输入（历史对话）：

> 我问你如何实现一个 gRPC 客户端，你给了我代码，并且解释了连接池。

压缩后：

> 用户正在实现 gRPC 客户端，已理解连接池相关内容。

- 模板：请将以下历史对话压缩为不超过 N 字的摘要，保留任务目标、重要结论、未完成事项。

适用场景：

- 多轮长对话压缩
- 保留语义即可
- 需要让模型继续推理

------

## 2.2 摘要压缩（Extractive Compression）

将冗长的文档或对话记录概括成模型可以处理的简短形式。然后，将摘要作为上下文提供给模型。虽然这种方法可以捕捉关键信息，但也引入了额外的出错风险。例如，糟糕的摘要可能会遗漏关键细节，甚至引入不准确之处。此外，为每种潜在上下文即时生成高质量的摘要也相当耗时。

  ![](https://www.dailydoseofds.com/content/images/2025/05/summarizer.svg)

- 关键参数
- API schema
- 用户需求
- 决策点

模板：从历史内容中抽取所有必需的事实、参数、代码、配置，不改写、不推理，只做抽取。

适用场景：

- 文档型、技术讨论
- 必须 **保留原文精确性**（如代码、API）
- 对模型生成不信任

- 摘要：另一种策略是

​            	

------

## 2.3 选择性压缩（Selective Compression）

保留：当前任务相关内容；丢弃：不相关历史、已完成部分、扩展讨论

流程类似：历史对话 → 相关性评估 → 仅保留相关内容

模板：从历史中仅保留与"当前任务"相关的内容，其余完全丢弃。不要尝试总结不相关内容。

适用场景：

- 多任务混合
- 长期会话
- 大量无关闲聊或噪音

------

## 2.4 结构化压缩（Structural Compression）

将“杂乱对话”变成“结构化信息”。

模板

```html
请将历史对话结构化为以下格式：
# Goals
# Constraints
# Known Info
# Decisions
# Todo

保持内容精准，不添加未提供的信息。
```

适用场景：

- 项目式对话
- 工作流对话
- Agent 系统

它是最稳、最安全的压缩方式。
